{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from auxiliary import ScaledEmbedding, ZeroEmbedding\n",
    "import data_loader\n",
    "import itertools\n",
    "\n",
    "class PT(nn.Module):\n",
    "    def get_params(self, users, items):\n",
    "        return self.al\n",
    "    def __init__(self, userLen, itemLen, distribution, params, item_price):\n",
    "        super(PT, self).__init__()\n",
    "        self.userNum = userLen\n",
    "        self.itemNum = itemLen\n",
    "        self.params = params\n",
    "\n",
    "        if 'gpu' in params and params['gpu'] == True:\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        l_size = params['l_size']\n",
    "        self.distribution = torch.FloatTensor(distribution).to(self.device)\n",
    "        self.item_price = torch.FloatTensor(item_price).to(self.device)\n",
    "        self.globalBias_g = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_g.weight.data += 0.5\n",
    "        self.globalBias_g.weight.requires_grad = False\n",
    "        self.ecoBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_g = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_g = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.ecoEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_d = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_d.weight.data += 0.5\n",
    "        self.globalBias_d.weight.requires_grad = False\n",
    "        self.ecoBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_d = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_d = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.ecoEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_a = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_a.weight.requires_grad = False\n",
    "        self.userBias_a = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.ecoBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_a = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.ecoEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.ecoEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.globalBias_b = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_b.weight.requires_grad = False\n",
    "        self.userBias_b = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.ecoBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.ecoBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_b = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.ecoEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.ecoEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.globalBias_l = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_l.weight.data += 1\n",
    "        self.globalBias_l.weight.requires_grad = False\n",
    "        self.userBias_l = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.ecoBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.ecoBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_l = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.ecoEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.ecoEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.reference_point = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.reference_point.weight.data = torch.ones_like(self.reference_point.weight.data) * 1.5\n",
    "        #\t\t self.reference_point.weight.requires_grad=False\n",
    "        self.to(self.device)\n",
    "        self.grads = {}\n",
    "    def ecoForward(self, items):\n",
    "        ecoBias_a = self.ecoBias_a(items)\n",
    "        ecoEmbed_a = self.ecoEmbed_a(items)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        alpha = ecoBias_a + torch.mul(ecoEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        distribution = self.distribution[items].to(self.device)\n",
    "        reference_point = self.reference_point(users)\n",
    "        #\t\t print(users.shape[0],items.shape[0])\n",
    "        price = self.item_price[items].view(-1, 1).expand(users.shape[0], 5).to(self.device)\n",
    "\n",
    "        # calculate value\n",
    "        globalBias_a = self.globalBias_a(torch.tensor(0).to(self.device))\n",
    "        userBias_a = self.userBias_a(users)\n",
    "        itemBias_a = self.itemBias_a(items)\n",
    "        userEmbed_a = self.userEmbed_a(users)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        globalBias_b = self.globalBias_b(torch.tensor(0).to(self.device))\n",
    "        userBias_b = self.userBias_b(users)\n",
    "        itemBias_b = self.itemBias_b(items)\n",
    "        userEmbed_b = self.userEmbed_b(users)\n",
    "        itemEmbed_b = self.itemEmbed_b(items)\n",
    "\n",
    "        globalBias_l = self.globalBias_l(torch.tensor(0).to(self.device))\n",
    "        userBias_l = self.userBias_l(users)\n",
    "        itemBias_l = self.itemBias_l(items)\n",
    "        userEmbed_l = self.userEmbed_l(users)\n",
    "        itemEmbed_l = self.itemEmbed_l(items)\n",
    "\n",
    "        alpha = globalBias_a + userBias_a + itemBias_a + torch.mul(userEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        beta = globalBias_b + userBias_b + itemBias_b + torch.mul(userEmbed_b, itemEmbed_b).sum(1).view(-1, 1)\n",
    "        lamda = globalBias_l + userBias_l + itemBias_l + torch.mul(userEmbed_l, itemEmbed_l).sum(1).view(-1, 1)\n",
    "\n",
    "        rating = torch.tensor([1., 2., 3., 4., 5.]).expand(users.shape[0], 5).to(self.device)\n",
    "        x = torch.tanh(rating - reference_point)\n",
    "        x_binary_pos = torch.gt(x, torch.FloatTensor([0]).to(self.device)).to(torch.float)\n",
    "        x_binary_neg = torch.ones_like(x).to(self.device) - x_binary_pos\n",
    "\n",
    "        x_ = torch.mul(price, torch.abs(x))\n",
    "        v_exp = torch.mul(alpha, x_binary_pos) + torch.mul(beta, x_binary_neg)\n",
    "        v = x_.pow(v_exp)\n",
    "        v_coef = x_binary_pos - torch.mul(lamda, x_binary_neg)\n",
    "        value = torch.mul(v, v_coef).to(self.device)\n",
    "\n",
    "        # calculate weight\n",
    "        globalBias_g = self.globalBias_g(torch.tensor(0).to(self.device))\n",
    "        userBias_g = self.userBias_g(users)\n",
    "        itemBias_g = self.itemBias_g(items)\n",
    "        userEmbed_g = self.userEmbed_g(users)\n",
    "        itemEmbed_g = self.itemEmbed_g(items)\n",
    "\n",
    "        globalBias_d = self.globalBias_d(torch.tensor(0).to(self.device))\n",
    "        userBias_d = self.userBias_d(users)\n",
    "        itemBias_d = self.itemBias_d(items)\n",
    "        userEmbed_d = self.userEmbed_d(users)\n",
    "        itemEmbed_d = self.itemEmbed_d(items)\n",
    "\n",
    "        gamma = globalBias_g + userBias_g + itemBias_g + torch.mul(userEmbed_g, itemEmbed_g).sum(1).view(-1, 1)\n",
    "        delta = globalBias_d + userBias_d + itemBias_d + torch.mul(userEmbed_d, itemEmbed_d).sum(1).view(-1, 1)\n",
    "\n",
    "        gamma_ = gamma.expand(users.shape[0], 5)\n",
    "        delta_ = delta.expand(users.shape[0], 5)\n",
    "        w_exp = torch.mul(x_binary_pos, gamma_) + torch.mul(x_binary_neg, delta_)\n",
    "\n",
    "        w_nominator = distribution.pow(w_exp)\n",
    "        w_denominator = (distribution.pow(w_exp) + (torch.ones_like(distribution).to(self.device) - distribution).pow(\n",
    "            w_exp)).pow(1 / w_exp)\n",
    "        weight = torch.div(w_nominator, w_denominator)\n",
    "\n",
    "        #\t\t self.userBias_g.weight.register_hook(self.save_grad('userBias_g'))\n",
    "        #\t\t self.itemBias_g.weight.register_hook(self.save_grad('itemBias_g'))\n",
    "        #\t\t self.userEmbed_g.weight.register_hook(self.save_grad('userEmbed_g'))\n",
    "        #\t\t self.itemEmbed_g.weight.register_hook(self.save_grad('itemEmbed_g'))\n",
    "        return torch.mul(weight, value).sum(1)\n",
    "\n",
    "    def getValues(self, users, items):\n",
    "        pOut = float(self.forward(users, items))  # .expand(users.shape[0], self.params['negNum_train']).reshape(-1, 1)\n",
    "        return pOut\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.grads\n",
    "\n",
    "    def save_grad(self, name):\n",
    "        def hook(grad):\n",
    "            self.grads[name] = grad\n",
    "\n",
    "        return hook\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    params = dict()\n",
    "    params['lr'] = 1e-4\n",
    "    params['batch_size'] = 1\n",
    "    params['epoch_limit'] = 1000\n",
    "    params['epoch_diff'] = 999\n",
    "    params['w_decay'] = 5e-4\n",
    "    params['negNum_test'] = 36\n",
    "    params['epsilon'] = 1e-4\n",
    "    params['negNum_train'] = 2\n",
    "    params['l_size'] = 128\n",
    "    params['train_device'] = 'cpu'\n",
    "    params['test_device'] = 'cpu'\n",
    "    params['lambda'] = 1.\n",
    "    params['test_per_train'] = 1\n",
    "    item_price = np.load(r\"C:\\\\Users\\march\\Risk-Aware-Recommnedation-Model\\data\\Movielens1M_item_price.npy\")\n",
    "    category1 = 'newTrainSamples'\n",
    "    category2 = 'newTestSamples'\n",
    "    catAll = 'AllSamples'\n",
    "    metaCat = 'Appliances'\n",
    "\n",
    "    train, test = data_loader.read_data(category1, category2)\n",
    "    userNum, itemNum = data_loader.get_datasize(catAll)\n",
    "    data_loader.get_ecoScores('Appliances', catAll)\n",
    "    AllSamples = data_loader.read_AllSamples(catAll)\n",
    "    frequency = data_loader.get_itemDist(AllSamples, itemNum)\n",
    "    distribution = data_loader.approx_Gaussian(frequency)\n",
    "\n",
    "    trainset = data_loader.TransactionData(train, userNum, itemNum, frequency)\n",
    "    trainLoader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "    trainset = data_loader.TransactionData(train, userNum, itemNum, frequency)\n",
    "    trainLoader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "    model = PT(userLen=userNum, itemLen=itemNum, distribution=distribution, params=params, item_price=item_price)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "\n",
    "    item_price = np.load(r\"C:\\\\Users\\march\\Risk-Aware-Recommnedation-Model\\data\\Movielens1M_item_price.npy\")\n",
    "    category1 = 'newTrainSamples'\n",
    "    category2 = 'newTestSamples'\n",
    "    catAll = 'AllSamples'\n",
    "    metaCat = 'Appliances'\n",
    "\n",
    "    train, test = data_loader.read_data(category1, category2)\n",
    "    userNum, itemNum = data_loader.get_datasize(catAll)\n",
    "    data_loader.get_ecoScores(metaCat, catAll)\n",
    "    AllSamples = data_loader.read_AllSamples(catAll)\n",
    "    frequency = data_loader.get_itemDist(AllSamples, itemNum)\n",
    "    distribution = data_loader.approx_Gaussian(frequency)\n",
    "    meta = data_loader.ecoList\n",
    "\n",
    "    trainset = data_loader.TransactionData(train, userNum, itemNum, frequency)\n",
    "    trainLoader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "    trainset = data_loader.TransactionData(train, userNum, itemNum, frequency)\n",
    "    trainLoader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "    model = PT(userLen=userNum, itemLen=itemNum, distribution=distribution, params=params, item_price=item_price)\n",
    "\n",
    "    model.device = params['train_device']\n",
    "    model.to(model.device)\n",
    "\n",
    "    values = []\n",
    "    L = len(trainLoader.dataset)\n",
    "    for i, batchData in enumerate(trainLoader):\n",
    "        users = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        items = torch.LongTensor(batchData['item']).to(model.device)\n",
    "        values.append(model.getValues(users,items))\n",
    "    xTrain = []\n",
    "    xTest = []\n",
    "    yTrain = []\n",
    "    ecoYTrain = []\n",
    "    yTest = []\n",
    "    ecoYTest = []\n",
    "\n",
    "    idx = 0\n",
    "    for x in range(len(train)):\n",
    "        xTrain.append([x for x in distribution[train[x][1]]])\n",
    "        xTrain[idx].append(values[idx])\n",
    "        xTrain[idx].append([x for x in OneHotEncoder().fit_transform([meta[train[x][1]][0].split()]).data])\n",
    "        yTrain.append(train[x][2])\n",
    "        for k in data_loader.items:\n",
    "            if data_loader.items[k][1] == train[x][1]:\n",
    "                ecoYTrain.append(data_loader.items[k][0])\n",
    "        idx += 1\n",
    "    idx = 0\n",
    "    for x in range(len(test)):\n",
    "        xTest.append([x for x in distribution[test[x][1]]])\n",
    "        xTest[idx].append(values[idx])\n",
    "        xTest[idx].append([x for x in OneHotEncoder().fit_transform([meta[test[x][1]][0].split()]).data])\n",
    "        yTest.append(test[x][2])\n",
    "        for k in data_loader.items:\n",
    "            if data_loader.items[k][1] == test[x][1]:\n",
    "                ecoYTest.append(data_loader.items[k][0])\n",
    "        idx += 1\n",
    "    flat = []\n",
    "    idx = 0\n",
    "    max = len(xTrain[0])\n",
    "    for x in xTrain:\n",
    "        for j in x:\n",
    "            if type(j) == list:\n",
    "                for k in j:\n",
    "                    flat.append(k)\n",
    "            else:\n",
    "                flat.append(j)\n",
    "        xTrain[idx] = flat.copy()\n",
    "        if len(xTrain[idx]) > max:\n",
    "            max = len(xTrain[idx])\n",
    "        flat = []\n",
    "        idx += 1\n",
    "    for item in range(len(xTrain)):\n",
    "        if len(xTrain[item]) < max:\n",
    "            for k in range(max - len(xTrain[item])):\n",
    "                xTrain[item].append(0.0)\n",
    "    idx = 0\n",
    "    for x in xTest:\n",
    "        for j in x:\n",
    "            if type(j) == list:\n",
    "                for k in j:\n",
    "                    flat.append(k)\n",
    "            else:\n",
    "                flat.append(j)\n",
    "        xTest[idx] = flat.copy()\n",
    "        flat = []\n",
    "        idx += 1\n",
    "    for item in range(len(xTest)):\n",
    "        if len(xTest[item]) < max:\n",
    "            for k in range(max - len(xTest[item])):\n",
    "                xTest[item].append(0.0)\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric= 'minkowski', p=2)\n",
    "    print(\"Utility epoch limit: \" + str(params['epoch_limit']))\n",
    "    for i in range(params['epoch_limit']):\n",
    "        knn.fit(xTrain, yTrain)\n",
    "    #print(knn.kneighbors)\n",
    "    for j in range(params['epoch_limit']-params['epoch_diff']):\n",
    "        knn.fit(xTrain, ecoYTrain)\n",
    "    print(\"Utility: \")\n",
    "    print(knn.score(xTest, yTest))\n",
    "    print(\"Eco epoch limit: \" + str(params['epoch_limit']-params['epoch_diff']))\n",
    "    print(\"Eco-friendliness: \")\n",
    "    print(knn.score(xTest, ecoYTest))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
