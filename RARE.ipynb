{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "finnish-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pdb\n",
    "from heapq import heappush, heappop\n",
    "from auxiliary import ScaledEmbedding, ZeroEmbedding\n",
    "import evaluation\n",
    "import data_loader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "#from cpt.cpt import Cpt\n",
    "#from cpt import PT_LogSoftmax\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "vulnerable-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "    params = dict()\n",
    "    params['lr'] = 1e-4\n",
    "    params['batch_size'] = 1\n",
    "    params['epoch_limit'] = 50\n",
    "    params['w_decay'] = 5e-4\n",
    "    params['negNum_test'] = 36\n",
    "    params['epsilon'] = 1e-4\n",
    "    params['negNum_train'] = 2\n",
    "    params['l_size'] = 16\n",
    "    params['train_device'] = 'cpu'\n",
    "    params['test_device'] = 'cpu'\n",
    "    params['lambda'] = 1.\n",
    "    params['test_per_train'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "minus-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "    item_price = np.load(r\"C:\\\\Users\\march\\Risk-Aware-Recommnedation-Model\\data\\Movielens1M_item_price.npy\")\n",
    "    category1 = 'newTrainSamples'\n",
    "    category2 = 'newTestSamples'\n",
    "    catAll = 'AllSamples'\n",
    "    metaCat = 'Appliances'\n",
    "    \n",
    "    train, test = data_loader.read_data(category1, category2)\n",
    "    userNum, itemNum = data_loader.get_datasize(catAll)\n",
    "    data_loader.get_ecoScores(metaCat, catAll)\n",
    "    AllSamples = data_loader.read_AllSamples(catAll)\n",
    "    distribution = data_loader.get_itemDist(AllSamples, itemNum)\n",
    "    distribution = data_loader.approx_Gaussian(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "falling-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PT(nn.Module):\n",
    "    def __init__(self, userLen, itemLen, distribution, params, item_price):\n",
    "        super(PT, self).__init__()\n",
    "        self.userNum = userLen\n",
    "        self.itemNum = itemLen\n",
    "        self.params = params\n",
    "\n",
    "        if 'gpu' in params and params['gpu'] == True:\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        l_size = params['l_size']\n",
    "        self.distribution = torch.FloatTensor(distribution).to(self.device)\n",
    "        self.item_price = torch.FloatTensor(item_price).to(self.device)\n",
    "        self.globalBias_g = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_g.weight.data += 0.5\n",
    "        self.globalBias_g.weight.requires_grad = False\n",
    "        #self.ecoBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_g = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_g = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_d = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_d.weight.data += 0.5\n",
    "        self.globalBias_d.weight.requires_grad = False\n",
    "        #self.ecoBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_d = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_d = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_a = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_a.weight.requires_grad = False\n",
    "        self.userBias_a = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.ecoBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.ecoBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_a = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.ecoEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.ecoEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.globalBias_b = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_b.weight.requires_grad = False\n",
    "        self.userBias_b = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        #self.ecoBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        #self.ecoBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_b = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        #self.ecoEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        #self.ecoEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.globalBias_l = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_l.weight.data += 1\n",
    "        self.globalBias_l.weight.requires_grad = False\n",
    "        self.userBias_l = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        #self.ecoBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        #self.ecoBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_l = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        #self.ecoEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.reference_point = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.reference_point.weight.data = torch.ones_like(self.reference_point.weight.data) * 1.5\n",
    "        #\t\t self.reference_point.weight.requires_grad=False\n",
    "        self.to(self.device)\n",
    "        self.grads = {}\n",
    "        \n",
    "    def ecoForward(self, items):\n",
    "        ecoBias_a = self.ecoBias_a(items)\n",
    "        ecoEmbed_a = self.ecoEmbed_a(items)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        alpha = ecoBias_a + torch.mul(ecoEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        distribution = self.distribution[items].to(self.device)\n",
    "        reference_point = self.reference_point(users)\n",
    "        #\t\t print(users.shape[0],items.shape[0])\n",
    "        price = self.item_price[items].view(-1, 1).expand(users.shape[0], 5).to(self.device)\n",
    "\n",
    "        # calculate value\n",
    "        globalBias_a = self.globalBias_a(torch.tensor(0).to(self.device))\n",
    "        userBias_a = self.userBias_a(users)\n",
    "        itemBias_a = self.itemBias_a(items)\n",
    "        userEmbed_a = self.userEmbed_a(users)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        globalBias_b = self.globalBias_b(torch.tensor(0).to(self.device))\n",
    "        userBias_b = self.userBias_b(users)\n",
    "        itemBias_b = self.itemBias_b(items)\n",
    "        userEmbed_b = self.userEmbed_b(users)\n",
    "        itemEmbed_b = self.itemEmbed_b(items)\n",
    "\n",
    "        globalBias_l = self.globalBias_l(torch.tensor(0).to(self.device))\n",
    "        userBias_l = self.userBias_l(users)\n",
    "        itemBias_l = self.itemBias_l(items)\n",
    "        userEmbed_l = self.userEmbed_l(users)\n",
    "        itemEmbed_l = self.itemEmbed_l(items)\n",
    "\n",
    "        alpha = globalBias_a + userBias_a + itemBias_a + torch.mul(userEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        beta = globalBias_b + userBias_b + itemBias_b + torch.mul(userEmbed_b, itemEmbed_b).sum(1).view(-1, 1)\n",
    "        lamda = globalBias_l + userBias_l + itemBias_l + torch.mul(userEmbed_l, itemEmbed_l).sum(1).view(-1, 1)\n",
    "\n",
    "        rating = torch.tensor([1., 2., 3., 4., 5.]).expand(users.shape[0], 5).to(self.device)\n",
    "        x = torch.tanh(rating - reference_point)\n",
    "        x_binary_pos = torch.gt(x, torch.FloatTensor([0]).to(self.device)).to(torch.float)\n",
    "        x_binary_neg = torch.ones_like(x).to(self.device) - x_binary_pos\n",
    "\n",
    "        x_ = torch.mul(price, torch.abs(x))\n",
    "        v_exp = torch.mul(alpha, x_binary_pos) + torch.mul(beta, x_binary_neg)\n",
    "        v = x_.pow(v_exp)\n",
    "        v_coef = x_binary_pos - torch.mul(lamda, x_binary_neg)\n",
    "        value = torch.mul(v, v_coef).to(self.device)\n",
    "\n",
    "        # calculate weight\n",
    "        globalBias_g = self.globalBias_g(torch.tensor(0).to(self.device))\n",
    "        userBias_g = self.userBias_g(users)\n",
    "        itemBias_g = self.itemBias_g(items)\n",
    "        userEmbed_g = self.userEmbed_g(users)\n",
    "        itemEmbed_g = self.itemEmbed_g(items)\n",
    "\n",
    "        globalBias_d = self.globalBias_d(torch.tensor(0).to(self.device))\n",
    "        userBias_d = self.userBias_d(users)\n",
    "        itemBias_d = self.itemBias_d(items)\n",
    "        userEmbed_d = self.userEmbed_d(users)\n",
    "        itemEmbed_d = self.itemEmbed_d(items)\n",
    "\n",
    "        gamma = globalBias_g + userBias_g + itemBias_g + torch.mul(userEmbed_g, itemEmbed_g).sum(1).view(-1, 1)\n",
    "        delta = globalBias_d + userBias_d + itemBias_d + torch.mul(userEmbed_d, itemEmbed_d).sum(1).view(-1, 1)\n",
    "\n",
    "        gamma_ = gamma.expand(users.shape[0], 5)\n",
    "        delta_ = delta.expand(users.shape[0], 5)\n",
    "        w_exp = torch.mul(x_binary_pos, gamma_) + torch.mul(x_binary_neg, delta_)\n",
    "\n",
    "        w_nominator = distribution.pow(w_exp)\n",
    "        w_denominator = (distribution.pow(w_exp) + (torch.ones_like(distribution).to(self.device) - distribution).pow(\n",
    "            w_exp)).pow(1 / w_exp)\n",
    "        weight = torch.div(w_nominator, w_denominator)\n",
    "\n",
    "        #\t\t self.userBias_g.weight.register_hook(self.save_grad('userBias_g'))\n",
    "        #\t\t self.itemBias_g.weight.register_hook(self.save_grad('itemBias_g'))\n",
    "        #\t\t self.userEmbed_g.weight.register_hook(self.save_grad('userEmbed_g'))\n",
    "        #\t\t self.itemEmbed_g.weight.register_hook(self.save_grad('itemEmbed_g'))\n",
    "        return torch.mul(weight, value).sum(1)\n",
    "    \n",
    "    def loss(self, users, items, negItems):\n",
    "        nusers = users.view(-1, 1).to(self.device)\n",
    "        nusers = nusers.expand(nusers.shape[0], self.params['negNum_train']).reshape(-1).to(self.device)\n",
    "\n",
    "        pOut = self.forward(users, items).view(-1, 1)#.expand(users.shape[0], self.params['negNum_train']).reshape(-1, 1)\n",
    "        nOut = self.forward(nusers, negItems).reshape(-1, self.params['negNum_train'])\n",
    "        Out = torch.cat((pOut,nOut),dim=1)\n",
    "        \n",
    "#         print(Out.shape)\n",
    "#         print(nOut.shape)\n",
    "#         input()\n",
    "        criterion = nn.LogSoftmax(dim=1)\n",
    "        res = criterion(Out)[:,0]\n",
    "        loss = torch.mean(res)\n",
    "        neg = data_loader.get_env_neg(items, self.params['negNum_train'])\n",
    "        for j in data_loader.items:\n",
    "            if data_loader.items[j][1] == items and data_loader.items[j][0] == 1:\n",
    "                env = 1\n",
    "                break\n",
    "            elif data_loader.items[j][1] == items and data_loader.items[j][0] == 0:\n",
    "                env = 0\n",
    "                break\n",
    "        for n in range(len(neg)):\n",
    "            neg[n] = neg[n][1]\n",
    "        if env == 1:\n",
    "            Out = torch.cat((pOut, self.ecoForward(torch.tensor(neg)).reshape(-1, self.params['negNum_train'])), dim=1)\n",
    "            res = criterion(Out)[:, 0]\n",
    "            loss += torch.mean(res)\n",
    "        else:\n",
    "            Out = -torch.cat((pOut, self.ecoForward(torch.tensor(neg)).reshape(-1, self.params['negNum_train'])), dim=1)\n",
    "            res = criterion(Out)[:, 0]\n",
    "            loss += torch.mean(res)\n",
    "        return -loss\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.grads\n",
    "\n",
    "    def save_grad(self, name):\n",
    "        def hook(grad):\n",
    "            self.grads[name] = grad\n",
    "        return hook\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-attraction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch  1  training...\n",
      "processed: 1: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:07<00:00, 92.74it/s]\n",
      "epoch loss tensor(1663.2122, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:13<00:00, 13.44it/s]\n",
      "\tPrecision@: {1:0.010810810810810811; 5: 0.019459459459459465; 10: 0.022702702702702707; 20: 0.02567567567567563}\n",
      "\tRecall@: {1:0.010810810810810811; 5: 0.0972972972972973; 10: 0.22702702702702704; 20: 0.5135135135135135}\n",
      "\tF1@: {1:0.010810810810810811; 5: 0.03243243243243244; 10: 0.04127764127764129; 20: 0.04890604890604883}\n",
      "\tNDCG@: {1:0.010810810810810811; 5: 0.06819186752977267; 10: 0.11172487124889105; 20: 0.1856740618228841}\n",
      "\tPrecision@: {1:0.7783783783783784; 5: 0.8508108108108113; 10: 0.7767567567567571; 20: 0.6748648648648656}\n",
      "\tRecall@: {1:0.7783783783783784; 5: 4.254054054054054; 10: 7.767567567567568; 20: 13.497297297297298}\n",
      "\tF1@: {1:0.7783783783783784; 5: 1.4180180180180186; 10: 1.4122850122850128; 20: 1.2854568854568869}\n",
      "\tNDCG@: {1:0.7783783783783784; 5: 3.018902284997394; 10: 4.213612440117561; 20: 5.689063567045552}\n",
      "Epoch  2  training...\n",
      "processed: 2: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:08<00:00, 83.83it/s]\n",
      "epoch loss tensor(1545.6459, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:13<00:00, 13.36it/s]\n",
      "\tPrecision@: {1:0.005405405405405406; 5: 0.02378378378378379; 10: 0.024324324324324326; 20: 0.02486486486486482}\n",
      "\tRecall@: {1:0.005405405405405406; 5: 0.11891891891891893; 10: 0.24324324324324326; 20: 0.4972972972972973}\n",
      "\tF1@: {1:0.005405405405405406; 5: 0.03963963963963965; 10: 0.04422604422604423; 20: 0.04736164736164728}\n",
      "\tNDCG@: {1:0.005405405405405406; 5: 0.07006854230314348; 10: 0.11256254499535079; 20: 0.17753233533167825}\n",
      "\tPrecision@: {1:0.7891891891891892; 5: 0.9437837837837838; 10: 0.8589189189189199; 20: 0.6864864864864864}\n",
      "\tRecall@: {1:0.7891891891891892; 5: 4.718918918918919; 10: 8.58918918918919; 20: 13.72972972972973}\n",
      "\tF1@: {1:0.7891891891891892; 5: 1.5729729729729731; 10: 1.5616707616707635; 20: 1.3075933075933073}\n",
      "\tNDCG@: {1:0.7891891891891892; 5: 3.3161602296598187; 10: 4.63711173303388; 20: 5.959253769849947}\n",
      "Epoch  3  training...\n",
      "processed: 3: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:10<00:00, 73.15it/s]\n",
      "epoch loss tensor(1416.4904, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:14<00:00, 12.78it/s]\n",
      "\tPrecision@: {1:0.010810810810810811; 5: 0.019459459459459465; 10: 0.02108108108108109; 20: 0.024054054054054013}\n",
      "\tRecall@: {1:0.010810810810810811; 5: 0.0972972972972973; 10: 0.21081081081081082; 20: 0.4810810810810811}\n",
      "\tF1@: {1:0.010810810810810811; 5: 0.03243243243243244; 10: 0.038329238329238347; 20: 0.045817245817245736}\n",
      "\tNDCG@: {1:0.010810810810810811; 5: 0.06403883693553296; 10: 0.10307679245895973; 20: 0.17218835760272638}\n",
      "\tPrecision@: {1:0.8594594594594595; 5: 0.9718918918918921; 10: 0.9264864864864875; 20: 0.7045945945945948}\n",
      "\tRecall@: {1:0.8594594594594595; 5: 4.859459459459459; 10: 9.264864864864865; 20: 14.091891891891892}\n",
      "\tF1@: {1:0.8594594594594595; 5: 1.6198198198198202; 10: 1.684520884520886; 20: 1.3420849420849426}\n",
      "\tNDCG@: {1:0.8594594594594595; 5: 3.421065771104294; 10: 4.925518008763877; 20: 6.171296638546367}\n",
      "Epoch  4  training...\n",
      "processed: 4: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:11<00:00, 63.78it/s]\n",
      "epoch loss tensor(1263.4951, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:14<00:00, 12.62it/s]\n",
      "\tPrecision@: {1:0.005405405405405406; 5: 0.022702702702702707; 10: 0.025405405405405403; 20: 0.02459459459459455}\n",
      "\tRecall@: {1:0.005405405405405406; 5: 0.11351351351351352; 10: 0.25405405405405407; 20: 0.4918918918918919}\n",
      "\tF1@: {1:0.005405405405405406; 5: 0.03783783783783785; 10: 0.04619164619164619; 20: 0.04684684684684677}\n",
      "\tNDCG@: {1:0.005405405405405406; 5: 0.06939568500162983; 10: 0.11685509999165884; 20: 0.17679172663964476}\n",
      "\tPrecision@: {1:0.8972972972972973; 5: 0.9762162162162165; 10: 0.9205405405405418; 20: 0.6954054054054054}\n",
      "\tRecall@: {1:0.8972972972972973; 5: 4.881081081081081; 10: 9.205405405405406; 20: 13.908108108108108}\n",
      "\tF1@: {1:0.8972972972972975; 5: 1.6270270270270273; 10: 1.6737100737100759; 20: 1.3245817245817246}\n",
      "\tNDCG@: {1:0.8972972972972973; 5: 3.4511702221417355; 10: 4.928727389444913; 20: 6.145852415182614}\n",
      "Epoch  5  training...\n",
      "processed: 5: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:11<00:00, 62.69it/s]\n",
      "epoch loss tensor(1158.9055, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:14<00:00, 13.13it/s]\n",
      "\tPrecision@: {1:0.016216216216216217; 5: 0.022702702702702707; 10: 0.02594594594594594; 20: 0.027027027027026973}\n",
      "\tRecall@: {1:0.016216216216216217; 5: 0.11351351351351352; 10: 0.2594594594594595; 20: 0.5405405405405406}\n",
      "\tF1@: {1:0.016216216216216217; 5: 0.03783783783783785; 10: 0.04717444717444717; 20: 0.051480051480051386}\n",
      "\tNDCG@: {1:0.016216216216216217; 5: 0.08008512804597842; 10: 0.13009908207959392; 20: 0.20201012055963372}\n",
      "\tPrecision@: {1:0.8216216216216217; 5: 0.9600000000000007; 10: 0.9010810810810821; 20: 0.6940540540540541}\n",
      "\tRecall@: {1:0.8216216216216217; 5: 4.8; 10: 9.010810810810812; 20: 13.881081081081081}\n",
      "\tF1@: {1:0.8216216216216217; 5: 1.6000000000000012; 10: 1.6383292383292403; 20: 1.322007722007722}\n",
      "\tNDCG@: {1:0.8216216216216217; 5: 3.3735412863787797; 10: 4.812439631844923; 20: 6.062804027985306}\n",
      "Epoch  6  training...\n",
      "processed: 6: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:10<00:00, 70.37it/s]\n",
      "epoch loss tensor(1081.0836, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:14<00:00, 13.18it/s]\n",
      "\tPrecision@: {1:0.016216216216216217; 5: 0.01837837837837838; 10: 0.025405405405405403; 20: 0.025945945945945896}\n",
      "\tRecall@: {1:0.016216216216216217; 5: 0.0918918918918919; 10: 0.25405405405405407; 20: 0.518918918918919}\n",
      "\tF1@: {1:0.016216216216216217; 5: 0.030630630630630637; 10: 0.04619164619164619; 20: 0.049420849420849323}\n",
      "\tNDCG@: {1:0.016216216216216217; 5: 0.06785886043980051; 10: 0.1211249750280858; 20: 0.18884409663796534}\n",
      "\tPrecision@: {1:0.8648648648648649; 5: 0.9664864864864867; 10: 0.8967567567567576; 20: 0.6954054054054056}\n",
      "\tRecall@: {1:0.8648648648648649; 5: 4.832432432432433; 10: 8.967567567567567; 20: 13.908108108108108}\n",
      "\tF1@: {1:0.8648648648648649; 5: 1.610810810810811; 10: 1.6304668304668317; 20: 1.324581724581725}\n",
      "\tNDCG@: {1:0.8648648648648649; 5: 3.412128566832041; 10: 4.825697533089904; 20: 6.0974903733037475}\n",
      "Epoch  7  training...\n",
      "processed: 7: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:10<00:00, 70.95it/s]\n",
      "epoch loss tensor(1013.5876, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:14<00:00, 13.02it/s]\n",
      "\tPrecision@: {1:0.005405405405405406; 5: 0.020540540540540546; 10: 0.027567567567567557; 20: 0.025945945945945896}\n",
      "\tRecall@: {1:0.005405405405405406; 5: 0.10270270270270271; 10: 0.2756756756756757; 20: 0.518918918918919}\n",
      "\tF1@: {1:0.005405405405405406; 5: 0.034234234234234245; 10: 0.05012285012285011; 20: 0.049420849420849323}\n",
      "\tNDCG@: {1:0.005405405405405406; 5: 0.06748413913208912; 10: 0.12649575534863997; 20: 0.18907775212789618}\n",
      "\tPrecision@: {1:0.8810810810810811; 5: 0.9751351351351355; 10: 0.8962162162162172; 20: 0.6932432432432433}\n",
      "\tRecall@: {1:0.8810810810810811; 5: 4.875675675675676; 10: 8.962162162162162; 20: 13.864864864864865}\n",
      "\tF1@: {1:0.881081081081081; 5: 1.6252252252252257; 10: 1.6294840294840311; 20: 1.3204633204633205}\n",
      "\tNDCG@: {1:0.8810810810810811; 5: 3.4403594113309244; 10: 4.841135058338328; 20: 6.094455508508101}\n",
      "Epoch  8  training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 8: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:10<00:00, 67.98it/s]\n",
      "epoch loss tensor(977.3407, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      " 21%|████████████████▋                                                                | 38/185 [00:02<00:11, 13.25it/s]"
     ]
    }
   ],
   "source": [
    "trainset = data_loader.TransactionData(train, userNum, itemNum, distribution)\n",
    "trainLoader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "trainset = data_loader.TransactionData(train, userNum, itemNum, trainset.userHist)\n",
    "trainLoader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "testset = data_loader.UserTransactionData(test, userNum, itemNum, trainset.userHist)\n",
    "testset.set_negN(params['negNum_test'])\n",
    "testLoader = DataLoader(testset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "model = PT(userLen=userNum, itemLen=itemNum, distribution=distribution, params=params, item_price=item_price)\n",
    "# print('initialization', model.state_dict())\n",
    "# optimizer = optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['w_decay'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['w_decay'])\n",
    "\n",
    "epoch = 0\n",
    "print('start training...')\n",
    "while epoch < params['epoch_limit']:\n",
    "    model.device = params['train_device']\n",
    "    model.to(model.device)\n",
    "\n",
    "    epoch += 1\n",
    "    print('Epoch ', str(epoch), ' training...')\n",
    "    L = len(trainLoader.dataset)\n",
    "    pbar = tqdm(total = L, file=sys.stdout)\n",
    "    pbar.set_description('processed: %d' % epoch)\n",
    "    for i, batchData in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        users = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        items = torch.LongTensor(batchData['item']).to(model.device)\n",
    "        negItems = torch.LongTensor(batchData['negItem']).reshape(-1).to(model.device)\n",
    "        batch_loss = model.loss(users, items, negItems)\n",
    "        batch_loss.backward()\n",
    "        grads = model.get_grads()\n",
    "        \n",
    "#         print('userBias_g:',grads['userBias_g'])\n",
    "#         print('itemBias_g:',grads['itemBias_g'])\n",
    "#         print('userEmbed_g:',grads['userEmbed_g'])\n",
    "#         print('itemEmbed_g:',grads['itemEmbed_g'])\n",
    "#         input()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i == 0:\n",
    "            total_loss = batch_loss.clone()\n",
    "        else:\n",
    "            total_loss += batch_loss.clone()\n",
    "        pbar.update(users.shape[0])\n",
    "    pbar.close()\n",
    "    print('epoch loss', total_loss)\n",
    "#     print(model.state_dict())\n",
    "\n",
    "    if epoch % params['test_per_train'] == 0:\n",
    "        print('starting val...')\n",
    "        model.device = params['test_device']\n",
    "        model.to(model.device)\n",
    "        L = len(testLoader.dataset)\n",
    "        pbar = tqdm(total=L, file=sys.stdout)\n",
    "        with torch.no_grad():\n",
    "            scoreDict = dict()\n",
    "            ecoDict = dict()\n",
    "            for i, batchData in enumerate(testLoader):\n",
    "#                 if np.random.random() < 0.98:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "#                 if i%50 != 0:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "                user = torch.LongTensor(batchData['user']).to(model.device)\n",
    "                posItems = torch.LongTensor(batchData['posItem']).to(model.device)\n",
    "                negItems = torch.LongTensor(batchData['negItem']).to(model.device)\n",
    "\n",
    "                items = torch.cat((posItems, negItems), 1).view(-1)\n",
    "                users = user.expand(items.shape[0])\n",
    "\n",
    "                score = model.forward(users, items)\n",
    "                scoreHeap = list()\n",
    "                ecoHeap = list()\n",
    "                for j in range(score.shape[0]):\n",
    "                    gt = False\n",
    "                    et = False\n",
    "                    if j < posItems.shape[1]:\n",
    "                        gt = True\n",
    "                    for k in data_loader.items:\n",
    "                        if (data_loader.items[k][1] == items[j]):\n",
    "                            if data_loader.items[k][0] == 1:\n",
    "                                et = True\n",
    "                    heappush(scoreHeap, (1-score[j].cpu().numpy(), (0 + items[j].cpu().numpy(), gt)))\n",
    "                    heappush(ecoHeap, (1 - score[j].cpu().numpy(), (0 + items[j].cpu().numpy(), et)))\n",
    "                scores = list()\n",
    "                ecoScores = list()\n",
    "                candidate = len(scoreHeap)\n",
    "                for k in range(candidate):\n",
    "                    scores.append(heappop(scoreHeap))\n",
    "                    ecoScores.append(heappop(ecoHeap))\n",
    "                pbar.update(1)\n",
    "                scoreDict[user[0]] = (scores, posItems.shape[1])\n",
    "                ecoDict[user[0]] = (ecoScores, posItems.shape[1])\n",
    "        pbar.close()\n",
    "        testResult = evaluation.ranking_performance(scoreDict, params['negNum_test'])\n",
    "        testResult = evaluation.ranking_performance(ecoDict, params['negNum_test'])\n",
    "#         with open('./results/'+category+'/'+category+'_PT_valResult_'+str(epoch)+'.json', 'w') as outfile:\n",
    "#             json.dump(testResult, outfile)\n",
    "        \n",
    "print('starting test...')\n",
    "model.device = params['test_device']\n",
    "model.to(model.device)\n",
    "L = len(testLoader.dataset)\n",
    "pbar = tqdm(total=L, file=sys.stdout)\n",
    "with torch.no_grad():\n",
    "    scoreDict = dict()\n",
    "    ecoDict = dict()\n",
    "    for i, batchData in enumerate(testLoader):\n",
    "        user = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        posItems = torch.LongTensor(batchData['posItem']).to(model.device)\n",
    "        negItems = torch.LongTensor(batchData['negItem']).to(model.device)\n",
    "\n",
    "        items = torch.cat((posItems, negItems), 1).view(-1)\n",
    "        users = user.expand(items.shape[0])\n",
    "        score = model.forward(users, items)\n",
    "        scoreHeap = list()\n",
    "        ecoHeap = list()\n",
    "        for j in range(score.shape[0]):\n",
    "            gt = False\n",
    "            et = False\n",
    "            if j < posItems.shape[1]:\n",
    "                gt = True\n",
    "            for k in data_loader.items:\n",
    "                if (data_loader.items[k][1] == items[j]):\n",
    "                    if data_loader.items[k][0] == 1:\n",
    "                        et = True\n",
    "            heappush(scoreHeap, (1-score[j].cpu().numpy(), (0+items[j].cpu().numpy(), gt)))\n",
    "            heappush(ecoHeap, (1 - score[j].cpu().numpy(), (0 + items[j].cpu().numpy(), et)))\n",
    "        scores = list()\n",
    "        ecoScores = list()\n",
    "        candidate = len(scoreHeap)\n",
    "        for k in range(candidate):\n",
    "            scores.append(heappop(scoreHeap))\n",
    "            ecoScores.append(heappop(ecoHeap))\n",
    "        pbar.update(1)\n",
    "        scoreDict[int(user[0].cpu().numpy())] = (scores, posItems.shape[1])\n",
    "        ecoDict[user[0]] = (ecoScores, posItems.shape[1])\n",
    "pbar.close()\n",
    "testResult = evaluation.ranking_performance(scoreDict, params['negNum_test'])\n",
    "testResult = evaluation.ranking_performance(ecoDict, params['negNum_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-population",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
