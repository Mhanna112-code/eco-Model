{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "finnish-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pdb\n",
    "from heapq import heappush, heappop\n",
    "from auxiliary import ScaledEmbedding, ZeroEmbedding\n",
    "import evaluation\n",
    "import data_loader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "#from cpt.cpt import Cpt\n",
    "#from cpt import PT_LogSoftmax\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "sized-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "    params = dict()\n",
    "    params['lr'] = 1e-4\n",
    "    params['batch_size'] = 1\n",
    "    params['epoch_limit'] = 50\n",
    "    params['w_decay'] = 5e-4\n",
    "    params['negNum_test'] = 36\n",
    "    params['epsilon'] = 1e-4\n",
    "    params['negNum_train'] = 2\n",
    "    params['l_size'] = 16\n",
    "    params['train_device'] = 'cpu'\n",
    "    params['test_device'] = 'cpu'\n",
    "    params['lambda'] = 1.\n",
    "    params['test_per_train'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "elementary-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "    item_price = np.load(r\"C:\\\\Users\\march\\Risk-Aware-Recommnedation-Model\\data\\Movielens1M_item_price.npy\")\n",
    "    category1 = 'newTrainSamples'\n",
    "    category2 = 'newTestSamples'\n",
    "    catAll = 'AllSamples'\n",
    "    metaCat = 'Appliances'\n",
    "    \n",
    "    train, test = data_loader.read_data(category1, category2)\n",
    "    userNum, itemNum = data_loader.get_datasize(catAll)\n",
    "    data_loader.get_ecoScores(metaCat, catAll)\n",
    "    AllSamples = data_loader.read_AllSamples(catAll)\n",
    "    distribution = data_loader.get_itemDist(AllSamples, itemNum)\n",
    "    distribution = data_loader.approx_Gaussian(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "rough-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PT(nn.Module):\n",
    "    def __init__(self, userLen, itemLen, distribution, params, item_price):\n",
    "        super(PT, self).__init__()\n",
    "        self.userNum = userLen\n",
    "        self.itemNum = itemLen\n",
    "        self.params = params\n",
    "\n",
    "        if 'gpu' in params and params['gpu'] == True:\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        l_size = params['l_size']\n",
    "        self.distribution = torch.FloatTensor(distribution).to(self.device)\n",
    "        self.item_price = torch.FloatTensor(item_price).to(self.device)\n",
    "        self.globalBias_g = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_g.weight.data += 0.5\n",
    "        self.globalBias_g.weight.requires_grad = False\n",
    "        #self.ecoBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_g = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_g = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_d = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_d.weight.data += 0.5\n",
    "        self.globalBias_d.weight.requires_grad = False\n",
    "        #self.ecoBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_d = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_d = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_a = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_a.weight.requires_grad = False\n",
    "        self.userBias_a = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.ecoBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.ecoBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_a = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.ecoEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.ecoEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.globalBias_b = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_b.weight.requires_grad = False\n",
    "        self.userBias_b = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        #self.ecoBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        #self.ecoBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_b = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        #self.ecoEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        #self.ecoEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.globalBias_l = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_l.weight.data += 1\n",
    "        self.globalBias_l.weight.requires_grad = False\n",
    "        self.userBias_l = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        #self.ecoBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        #self.ecoBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_l = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        #self.ecoEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.reference_point = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.reference_point.weight.data = torch.ones_like(self.reference_point.weight.data) * 1.5\n",
    "        #\t\t self.reference_point.weight.requires_grad=False\n",
    "        self.to(self.device)\n",
    "        self.grads = {}\n",
    "        \n",
    "    def ecoForward(self, items):\n",
    "        ecoBias_a = self.ecoBias_a(items)\n",
    "        ecoEmbed_a = self.ecoEmbed_a(items)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        alpha = ecoBias_a + torch.mul(ecoEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        distribution = self.distribution[items].to(self.device)\n",
    "        reference_point = self.reference_point(users)\n",
    "        #\t\t print(users.shape[0],items.shape[0])\n",
    "        price = self.item_price[items].view(-1, 1).expand(users.shape[0], 5).to(self.device)\n",
    "\n",
    "        # calculate value\n",
    "        globalBias_a = self.globalBias_a(torch.tensor(0).to(self.device))\n",
    "        userBias_a = self.userBias_a(users)\n",
    "        itemBias_a = self.itemBias_a(items)\n",
    "        userEmbed_a = self.userEmbed_a(users)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        globalBias_b = self.globalBias_b(torch.tensor(0).to(self.device))\n",
    "        userBias_b = self.userBias_b(users)\n",
    "        itemBias_b = self.itemBias_b(items)\n",
    "        userEmbed_b = self.userEmbed_b(users)\n",
    "        itemEmbed_b = self.itemEmbed_b(items)\n",
    "\n",
    "        globalBias_l = self.globalBias_l(torch.tensor(0).to(self.device))\n",
    "        userBias_l = self.userBias_l(users)\n",
    "        itemBias_l = self.itemBias_l(items)\n",
    "        userEmbed_l = self.userEmbed_l(users)\n",
    "        itemEmbed_l = self.itemEmbed_l(items)\n",
    "\n",
    "        alpha = globalBias_a + userBias_a + itemBias_a + torch.mul(userEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        beta = globalBias_b + userBias_b + itemBias_b + torch.mul(userEmbed_b, itemEmbed_b).sum(1).view(-1, 1)\n",
    "        lamda = globalBias_l + userBias_l + itemBias_l + torch.mul(userEmbed_l, itemEmbed_l).sum(1).view(-1, 1)\n",
    "\n",
    "        rating = torch.tensor([1., 2., 3., 4., 5.]).expand(users.shape[0], 5).to(self.device)\n",
    "        x = torch.tanh(rating - reference_point)\n",
    "        x_binary_pos = torch.gt(x, torch.FloatTensor([0]).to(self.device)).to(torch.float)\n",
    "        x_binary_neg = torch.ones_like(x).to(self.device) - x_binary_pos\n",
    "\n",
    "        x_ = torch.mul(price, torch.abs(x))\n",
    "        v_exp = torch.mul(alpha, x_binary_pos) + torch.mul(beta, x_binary_neg)\n",
    "        v = x_.pow(v_exp)\n",
    "        v_coef = x_binary_pos - torch.mul(lamda, x_binary_neg)\n",
    "        value = torch.mul(v, v_coef).to(self.device)\n",
    "\n",
    "        # calculate weight\n",
    "        globalBias_g = self.globalBias_g(torch.tensor(0).to(self.device))\n",
    "        userBias_g = self.userBias_g(users)\n",
    "        itemBias_g = self.itemBias_g(items)\n",
    "        userEmbed_g = self.userEmbed_g(users)\n",
    "        itemEmbed_g = self.itemEmbed_g(items)\n",
    "\n",
    "        globalBias_d = self.globalBias_d(torch.tensor(0).to(self.device))\n",
    "        userBias_d = self.userBias_d(users)\n",
    "        itemBias_d = self.itemBias_d(items)\n",
    "        userEmbed_d = self.userEmbed_d(users)\n",
    "        itemEmbed_d = self.itemEmbed_d(items)\n",
    "\n",
    "        gamma = globalBias_g + userBias_g + itemBias_g + torch.mul(userEmbed_g, itemEmbed_g).sum(1).view(-1, 1)\n",
    "        delta = globalBias_d + userBias_d + itemBias_d + torch.mul(userEmbed_d, itemEmbed_d).sum(1).view(-1, 1)\n",
    "\n",
    "        gamma_ = gamma.expand(users.shape[0], 5)\n",
    "        delta_ = delta.expand(users.shape[0], 5)\n",
    "        w_exp = torch.mul(x_binary_pos, gamma_) + torch.mul(x_binary_neg, delta_)\n",
    "\n",
    "        w_nominator = distribution.pow(w_exp)\n",
    "        w_denominator = (distribution.pow(w_exp) + (torch.ones_like(distribution).to(self.device) - distribution).pow(\n",
    "            w_exp)).pow(1 / w_exp)\n",
    "        weight = torch.div(w_nominator, w_denominator)\n",
    "\n",
    "        #\t\t self.userBias_g.weight.register_hook(self.save_grad('userBias_g'))\n",
    "        #\t\t self.itemBias_g.weight.register_hook(self.save_grad('itemBias_g'))\n",
    "        #\t\t self.userEmbed_g.weight.register_hook(self.save_grad('userEmbed_g'))\n",
    "        #\t\t self.itemEmbed_g.weight.register_hook(self.save_grad('itemEmbed_g'))\n",
    "        return torch.mul(weight, value).sum(1)\n",
    "    \n",
    "    def loss(self, users, items, negItems):\n",
    "        nusers = users.view(-1, 1).to(self.device)\n",
    "        nusers = nusers.expand(nusers.shape[0], self.params['negNum_train']).reshape(-1).to(self.device)\n",
    "\n",
    "        pOut = self.forward(users, items).view(-1, 1)#.expand(users.shape[0], self.params['negNum_train']).reshape(-1, 1)\n",
    "        nOut = self.forward(nusers, negItems).reshape(-1, self.params['negNum_train'])\n",
    "        Out = torch.cat((pOut,nOut),dim=1)\n",
    "        \n",
    "#         print(Out.shape)\n",
    "#         print(nOut.shape)\n",
    "#         input()\n",
    "        criterion = nn.LogSoftmax(dim=1)\n",
    "        res = criterion(Out)[:,0]\n",
    "        loss = torch.mean(res)\n",
    "        neg = data_loader.get_env_neg(items, self.params['negNum_train'])\n",
    "        for j in data_loader.items:\n",
    "            if data_loader.items[j][1] == items and data_loader.items[j][0] == 1:\n",
    "                env = 1\n",
    "                break\n",
    "            elif data_loader.items[j][1] == items and data_loader.items[j][0] == 0:\n",
    "                env = 0\n",
    "                break\n",
    "        for n in range(len(neg)):\n",
    "            neg[n] = neg[n][1]\n",
    "        if env == 1:\n",
    "            Out = torch.cat((pOut, self.ecoForward(torch.tensor(neg)).reshape(-1, self.params['negNum_train'])), dim=1)\n",
    "            res = criterion(Out)[:, 0]\n",
    "            loss += torch.mean(res)\n",
    "        else:\n",
    "            Out = -torch.cat((pOut, self.ecoForward(torch.tensor(neg)).reshape(-1, self.params['negNum_train'])), dim=1)\n",
    "            res = criterion(Out)[:, 0]\n",
    "            loss += torch.mean(res)\n",
    "        return -loss\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.grads\n",
    "\n",
    "    def save_grad(self, name):\n",
    "        def hook(grad):\n",
    "            self.grads[name] = grad\n",
    "        return hook\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-output",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch  1  training...\n",
      "processed: 1: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:07<00:00, 98.70it/s]\n",
      "epoch loss tensor(1593.5339, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:13<00:00, 13.68it/s]\n",
      "\tPrecision@: {1:0.03783783783783784; 5: 0.02378378378378379; 10: 0.02162162162162163; 20: 0.023513513513513475}\n",
      "\tRecall@: {1:0.03783783783783784; 5: 0.11891891891891893; 10: 0.21621621621621623; 20: 0.4702702702702703}\n",
      "\tF1@: {1:0.03783783783783784; 5: 0.03963963963963965; 10: 0.039312039312039325; 20: 0.044787644787644715}\n",
      "\tNDCG@: {1:0.03783783783783784; 5: 0.09359864155949196; 10: 0.12664899624992668; 20: 0.19082268148541492}\n",
      "\tPrecision@: {1:0.8540540540540541; 5: 0.8972972972972976; 10: 0.8221621621621629; 20: 0.6794594594594595}\n",
      "\tRecall@: {1:0.8540540540540541; 5: 4.486486486486487; 10: 8.221621621621622; 20: 13.58918918918919}\n",
      "\tF1@: {1:0.854054054054054; 5: 1.4954954954954958; 10: 1.494840294840296; 20: 1.2942084942084942}\n",
      "\tNDCG@: {1:0.8540540540540541; 5: 3.2143328188777014; 10: 4.485770920699284; 20: 5.863526577530298}\n",
      "Epoch  2  training...\n",
      "processed: 2: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:08<00:00, 92.30it/s]\n",
      "epoch loss tensor(1488.1498, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 185/185 [00:13<00:00, 13.62it/s]\n",
      "\tPrecision@: {1:0.05405405405405406; 5: 0.030270270270270284; 10: 0.022162162162162168; 20: 0.023783783783783742}\n",
      "\tRecall@: {1:0.05405405405405406; 5: 0.15135135135135136; 10: 0.22162162162162163; 20: 0.4756756756756757}\n",
      "\tF1@: {1:0.05405405405405406; 5: 0.050450450450450476; 10: 0.0402948402948403; 20: 0.04530244530244523}\n",
      "\tNDCG@: {1:0.05405405405405406; 5: 0.11987622597109622; 10: 0.14460752586266343; 20: 0.2086642140595415}\n",
      "\tPrecision@: {1:0.8810810810810811; 5: 0.966486486486487; 10: 0.9086486486486497; 20: 0.7054054054054054}\n",
      "\tRecall@: {1:0.8810810810810811; 5: 4.832432432432433; 10: 9.086486486486486; 20: 14.108108108108109}\n",
      "\tF1@: {1:0.881081081081081; 5: 1.6108108108108115; 10: 1.6520884520884536; 20: 1.3436293436293436}\n",
      "\tNDCG@: {1:0.8810810810810811; 5: 3.4202366749401487; 10: 4.871680775005202; 20: 6.166065227870539}\n",
      "Epoch  3  training...\n",
      "processed: 3: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:10<00:00, 73.56it/s]\n",
      "epoch loss tensor(1348.0834, grad_fn=<AddBackward0>)\n",
      "starting val...\n",
      " 49%|███████████████████████████████████████▍                                         | 90/185 [00:06<00:06, 14.02it/s]"
     ]
    }
   ],
   "source": [
    "model = PT(userLen=userNum, itemLen=itemNum, distribution=distribution, params=params, item_price=item_price)\n",
    "# print('initialization', model.state_dict())\n",
    "# optimizer = optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['w_decay'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['w_decay'])\n",
    "\n",
    "epoch = 0\n",
    "print('start training...')\n",
    "while epoch < params['epoch_limit']:\n",
    "    model.device = params['train_device']\n",
    "    model.to(model.device)\n",
    "\n",
    "    epoch += 1\n",
    "    print('Epoch ', str(epoch), ' training...')\n",
    "    L = len(trainLoader.dataset)\n",
    "    pbar = tqdm(total = L, file=sys.stdout)\n",
    "    pbar.set_description('processed: %d' % epoch)\n",
    "    for i, batchData in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        users = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        items = torch.LongTensor(batchData['item']).to(model.device)\n",
    "        negItems = torch.LongTensor(batchData['negItem']).reshape(-1).to(model.device)\n",
    "        batch_loss = model.loss(users, items, negItems)\n",
    "        batch_loss.backward()\n",
    "        grads = model.get_grads()\n",
    "        \n",
    "#         print('userBias_g:',grads['userBias_g'])\n",
    "#         print('itemBias_g:',grads['itemBias_g'])\n",
    "#         print('userEmbed_g:',grads['userEmbed_g'])\n",
    "#         print('itemEmbed_g:',grads['itemEmbed_g'])\n",
    "#         input()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i == 0:\n",
    "            total_loss = batch_loss.clone()\n",
    "        else:\n",
    "            total_loss += batch_loss.clone()\n",
    "        pbar.update(users.shape[0])\n",
    "    pbar.close()\n",
    "    print('epoch loss', total_loss)\n",
    "#     print(model.state_dict())\n",
    "\n",
    "    if epoch % params['test_per_train'] == 0:\n",
    "        print('starting val...')\n",
    "        model.device = params['test_device']\n",
    "        model.to(model.device)\n",
    "        L = len(testLoader.dataset)\n",
    "        pbar = tqdm(total=L, file=sys.stdout)\n",
    "        with torch.no_grad():\n",
    "            scoreDict = dict()\n",
    "            ecoDict = dict()\n",
    "            for i, batchData in enumerate(testLoader):\n",
    "#                 if np.random.random() < 0.98:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "#                 if i%50 != 0:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "                user = torch.LongTensor(batchData['user']).to(model.device)\n",
    "                posItems = torch.LongTensor(batchData['posItem']).to(model.device)\n",
    "                negItems = torch.LongTensor(batchData['negItem']).to(model.device)\n",
    "\n",
    "                items = torch.cat((posItems, negItems), 1).view(-1)\n",
    "                users = user.expand(items.shape[0])\n",
    "\n",
    "                score = model.forward(users, items)\n",
    "                scoreHeap = list()\n",
    "                ecoHeap = list()\n",
    "                for j in range(score.shape[0]):\n",
    "                    gt = False\n",
    "                    et = False\n",
    "                    if j < posItems.shape[1]:\n",
    "                        gt = True\n",
    "                    for k in data_loader.items:\n",
    "                        if (data_loader.items[k][1] == items[j]):\n",
    "                            if data_loader.items[k][0] == 1:\n",
    "                                et = True\n",
    "                    heappush(scoreHeap, (1-score[j].cpu().numpy(), (0 + items[j].cpu().numpy(), gt)))\n",
    "                    heappush(ecoHeap, (1 - score[j].cpu().numpy(), (0 + items[j].cpu().numpy(), et)))\n",
    "                scores = list()\n",
    "                ecoScores = list()\n",
    "                candidate = len(scoreHeap)\n",
    "                for k in range(candidate):\n",
    "                    scores.append(heappop(scoreHeap))\n",
    "                    ecoScores.append(heappop(ecoHeap))\n",
    "                pbar.update(1)\n",
    "                scoreDict[user[0]] = (scores, posItems.shape[1])\n",
    "                ecoDict[user[0]] = (ecoScores, posItems.shape[1])\n",
    "        pbar.close()\n",
    "        testResult = evaluation.ranking_performance(scoreDict, params['negNum_test'])\n",
    "        testResult = evaluation.ranking_performance(ecoDict, params['negNum_test'])\n",
    "#         with open('./results/'+category+'/'+category+'_PT_valResult_'+str(epoch)+'.json', 'w') as outfile:\n",
    "#             json.dump(testResult, outfile)\n",
    "        \n",
    "print('starting test...')\n",
    "model.device = params['test_device']\n",
    "model.to(model.device)\n",
    "L = len(testLoader.dataset)\n",
    "pbar = tqdm(total=L, file=sys.stdout)\n",
    "with torch.no_grad():\n",
    "    scoreDict = dict()\n",
    "    ecoDict = dict()\n",
    "    for i, batchData in enumerate(testLoader):\n",
    "        user = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        posItems = torch.LongTensor(batchData['posItem']).to(model.device)\n",
    "        negItems = torch.LongTensor(batchData['negItem']).to(model.device)\n",
    "\n",
    "        items = torch.cat((posItems, negItems), 1).view(-1)\n",
    "        users = user.expand(items.shape[0])\n",
    "        score = model.forward(users, items)\n",
    "        scoreHeap = list()\n",
    "        ecoHeap = list()\n",
    "        for j in range(score.shape[0]):\n",
    "            gt = False\n",
    "            et = False\n",
    "            if j < posItems.shape[1]:\n",
    "                gt = True\n",
    "            for k in data_loader.items:\n",
    "                if (data_loader.items[k][1] == items[j]):\n",
    "                    if data_loader.items[k][0] == 1:\n",
    "                        et = True\n",
    "            heappush(scoreHeap, (1-score[j].cpu().numpy(), (0+items[j].cpu().numpy(), gt)))\n",
    "            heappush(ecoHeap, (1 - score[j].cpu().numpy(), (0 + items[j].cpu().numpy(), et)))\n",
    "        scores = list()\n",
    "        ecoScores = list()\n",
    "        candidate = len(scoreHeap)\n",
    "        for k in range(candidate):\n",
    "            scores.append(heappop(scoreHeap))\n",
    "            ecoScores.append(heappop(ecoHeap))\n",
    "        pbar.update(1)\n",
    "        scoreDict[int(user[0].cpu().numpy())] = (scores, posItems.shape[1])\n",
    "        ecoDict[user[0]] = (ecoScores, posItems.shape[1])\n",
    "pbar.close()\n",
    "testResult = evaluation.ranking_performance(scoreDict, params['negNum_test'])\n",
    "testResult = evaluation.ranking_performance(ecoDict, params['negNum_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-cemetery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
