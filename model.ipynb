{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "finnish-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pdb\n",
    "from heapq import heappush, heappop\n",
    "from auxiliary import ScaledEmbedding, ZeroEmbedding\n",
    "import evaluation\n",
    "import data_loader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "#from cpt.cpt import Cpt\n",
    "#from cpt import PT_LogSoftmax\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "blessed-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "    params = dict()\n",
    "    params['lr'] = 1e-4\n",
    "    params['batch_size'] = 1\n",
    "    params['epoch_limit'] = 50\n",
    "    params['w_decay'] = 5e-4\n",
    "    params['negNum_test'] = 36\n",
    "    params['epsilon'] = 1e-4\n",
    "    params['negNum_train'] = 2\n",
    "    params['l_size'] = 16\n",
    "    params['train_device'] = 'cpu'\n",
    "    params['test_device'] = 'cpu'\n",
    "    params['lambda'] = 1.\n",
    "    params['test_per_train'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "blocked-offer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1:   0%|                                                                            | 0/740 [05:07<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "    item_price = np.load(r\"C:\\\\Users\\march\\Risk-Aware-Recommnedation-Model\\data\\Movielens1M_item_price.npy\")\n",
    "    category1 = 'newTrainSamples'\n",
    "    category2 = 'newTestSamples'\n",
    "    catAll = 'AllSamples'\n",
    "    metaCat = 'Appliances'\n",
    "    \n",
    "    train, test = data_loader.read_data(category1, category2)\n",
    "    userNum, itemNum = data_loader.get_datasize(catAll)\n",
    "    data_loader.get_ecoScores(metaCat, catAll)\n",
    "    AllSamples = data_loader.read_AllSamples(catAll)\n",
    "    distribution = data_loader.get_itemDist(AllSamples, itemNum)\n",
    "    distribution = data_loader.approx_Gaussian(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "biological-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PT(nn.Module):\n",
    "    def __init__(self, userLen, itemLen, distribution, params, item_price):\n",
    "        super(PT, self).__init__()\n",
    "        self.userNum = userLen\n",
    "        self.itemNum = itemLen\n",
    "        self.params = params\n",
    "\n",
    "        if 'gpu' in params and params['gpu'] == True:\n",
    "            self.device = 'cuda'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "\n",
    "        l_size = params['l_size']\n",
    "        self.distribution = torch.FloatTensor(distribution).to(self.device)\n",
    "        self.item_price = torch.FloatTensor(item_price).to(self.device)\n",
    "        self.globalBias_g = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_g.weight.data += 0.5\n",
    "        self.globalBias_g.weight.requires_grad = False\n",
    "        #self.ecoBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_g = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_g = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_g = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_g = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_d = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_d.weight.data += 0.5\n",
    "        self.globalBias_d.weight.requires_grad = False\n",
    "        #self.ecoBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_d = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_d = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userEmbed_d = ScaledEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_d = ScaledEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "\n",
    "        self.globalBias_a = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_a.weight.requires_grad = False\n",
    "        self.userBias_a = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.ecoBias_a = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.ecoBias_a.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_a = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.ecoEmbed_a = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.ecoEmbed_a.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.globalBias_b = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_b.weight.requires_grad = False\n",
    "        self.userBias_b = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        #self.ecoBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        #self.ecoBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_b = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_b.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_b = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "        #self.ecoEmbed_b = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_b.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        #self.ecoEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.globalBias_l = ZeroEmbedding(1, 1).to(self.device).to(torch.float)\n",
    "        self.globalBias_l.weight.data += 1\n",
    "        self.globalBias_l.weight.requires_grad = False\n",
    "        self.userBias_l = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.userBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.itemBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        self.itemBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        #self.ecoBias_l = ZeroEmbedding(itemLen, 1).to(self.device).to(torch.float)\n",
    "        #self.ecoBias_l.weight.data.uniform_(0.0, 0.05)\n",
    "        self.userEmbed_l = ZeroEmbedding(userLen, l_size).to(self.device).to(torch.float)\n",
    "        self.userEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        self.itemEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        self.itemEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "        #self.ecoEmbed_l = ZeroEmbedding(itemLen, l_size).to(self.device).to(torch.float)\n",
    "        #self.ecoEmbed_l.weight.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "        self.reference_point = ZeroEmbedding(userLen, 1).to(self.device).to(torch.float)\n",
    "        self.reference_point.weight.data = torch.ones_like(self.reference_point.weight.data) * 1.5\n",
    "        #\t\t self.reference_point.weight.requires_grad=False\n",
    "        self.to(self.device)\n",
    "        self.grads = {}\n",
    "        \n",
    "    def ecoForward(self, items):\n",
    "        ecoBias_a = self.ecoBias_a(items)\n",
    "        ecoEmbed_a = self.ecoEmbed_a(items)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        alpha = ecoBias_a + torch.mul(ecoEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        return alpha\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        distribution = self.distribution[items].to(self.device)\n",
    "        reference_point = self.reference_point(users)\n",
    "        #\t\t print(users.shape[0],items.shape[0])\n",
    "        price = self.item_price[items].view(-1, 1).expand(users.shape[0], 5).to(self.device)\n",
    "\n",
    "        # calculate value\n",
    "        globalBias_a = self.globalBias_a(torch.tensor(0).to(self.device))\n",
    "        userBias_a = self.userBias_a(users)\n",
    "        itemBias_a = self.itemBias_a(items)\n",
    "        userEmbed_a = self.userEmbed_a(users)\n",
    "        itemEmbed_a = self.itemEmbed_a(items)\n",
    "\n",
    "        globalBias_b = self.globalBias_b(torch.tensor(0).to(self.device))\n",
    "        userBias_b = self.userBias_b(users)\n",
    "        itemBias_b = self.itemBias_b(items)\n",
    "        userEmbed_b = self.userEmbed_b(users)\n",
    "        itemEmbed_b = self.itemEmbed_b(items)\n",
    "\n",
    "        globalBias_l = self.globalBias_l(torch.tensor(0).to(self.device))\n",
    "        userBias_l = self.userBias_l(users)\n",
    "        itemBias_l = self.itemBias_l(items)\n",
    "        userEmbed_l = self.userEmbed_l(users)\n",
    "        itemEmbed_l = self.itemEmbed_l(items)\n",
    "\n",
    "        alpha = globalBias_a + userBias_a + itemBias_a + torch.mul(userEmbed_a, itemEmbed_a).sum(1).view(-1, 1)\n",
    "        beta = globalBias_b + userBias_b + itemBias_b + torch.mul(userEmbed_b, itemEmbed_b).sum(1).view(-1, 1)\n",
    "        lamda = globalBias_l + userBias_l + itemBias_l + torch.mul(userEmbed_l, itemEmbed_l).sum(1).view(-1, 1)\n",
    "\n",
    "        rating = torch.tensor([1., 2., 3., 4., 5.]).expand(users.shape[0], 5).to(self.device)\n",
    "        x = torch.tanh(rating - reference_point)\n",
    "        x_binary_pos = torch.gt(x, torch.FloatTensor([0]).to(self.device)).to(torch.float)\n",
    "        x_binary_neg = torch.ones_like(x).to(self.device) - x_binary_pos\n",
    "\n",
    "        x_ = torch.mul(price, torch.abs(x))\n",
    "        v_exp = torch.mul(alpha, x_binary_pos) + torch.mul(beta, x_binary_neg)\n",
    "        v = x_.pow(v_exp)\n",
    "        v_coef = x_binary_pos - torch.mul(lamda, x_binary_neg)\n",
    "        value = torch.mul(v, v_coef).to(self.device)\n",
    "\n",
    "        # calculate weight\n",
    "        globalBias_g = self.globalBias_g(torch.tensor(0).to(self.device))\n",
    "        userBias_g = self.userBias_g(users)\n",
    "        itemBias_g = self.itemBias_g(items)\n",
    "        userEmbed_g = self.userEmbed_g(users)\n",
    "        itemEmbed_g = self.itemEmbed_g(items)\n",
    "\n",
    "        globalBias_d = self.globalBias_d(torch.tensor(0).to(self.device))\n",
    "        userBias_d = self.userBias_d(users)\n",
    "        itemBias_d = self.itemBias_d(items)\n",
    "        userEmbed_d = self.userEmbed_d(users)\n",
    "        itemEmbed_d = self.itemEmbed_d(items)\n",
    "\n",
    "        gamma = globalBias_g + userBias_g + itemBias_g + torch.mul(userEmbed_g, itemEmbed_g).sum(1).view(-1, 1)\n",
    "        delta = globalBias_d + userBias_d + itemBias_d + torch.mul(userEmbed_d, itemEmbed_d).sum(1).view(-1, 1)\n",
    "\n",
    "        gamma_ = gamma.expand(users.shape[0], 5)\n",
    "        delta_ = delta.expand(users.shape[0], 5)\n",
    "        w_exp = torch.mul(x_binary_pos, gamma_) + torch.mul(x_binary_neg, delta_)\n",
    "\n",
    "        w_nominator = distribution.pow(w_exp)\n",
    "        w_denominator = (distribution.pow(w_exp) + (torch.ones_like(distribution).to(self.device) - distribution).pow(\n",
    "            w_exp)).pow(1 / w_exp)\n",
    "        weight = torch.div(w_nominator, w_denominator)\n",
    "\n",
    "        #\t\t self.userBias_g.weight.register_hook(self.save_grad('userBias_g'))\n",
    "        #\t\t self.itemBias_g.weight.register_hook(self.save_grad('itemBias_g'))\n",
    "        #\t\t self.userEmbed_g.weight.register_hook(self.save_grad('userEmbed_g'))\n",
    "        #\t\t self.itemEmbed_g.weight.register_hook(self.save_grad('itemEmbed_g'))\n",
    "        return torch.mul(weight, value).sum(1)\n",
    "    \n",
    "    def loss(self, users, items, negItems):\n",
    "        nusers = users.view(-1, 1).to(self.device)\n",
    "        nusers = nusers.expand(nusers.shape[0], self.params['negNum_train']).reshape(-1).to(self.device)\n",
    "\n",
    "        pOut = self.forward(users, items).view(-1, 1)#.expand(users.shape[0], self.params['negNum_train']).reshape(-1, 1)\n",
    "        nOut = self.forward(nusers, negItems).reshape(-1, self.params['negNum_train'])\n",
    "        Out = torch.cat((pOut,nOut),dim=1)\n",
    "        \n",
    "#         print(Out.shape)\n",
    "#         print(nOut.shape)\n",
    "#         input()\n",
    "        criterion = nn.LogSoftmax(dim=1)\n",
    "        res = criterion(Out)[:,0]\n",
    "        loss = torch.mean(res)\n",
    "        neg = data_loader.get_env_neg(items, self.params['negNum_train'])\n",
    "        for j in data_loader.items:\n",
    "            if data_loader.items[j][1] == items and data_loader.items[j][0] == 1:\n",
    "                env = 1\n",
    "                break\n",
    "            elif data_loader.items[j][1] == items and data_loader.items[j][0] == 0:\n",
    "                env = 0\n",
    "                break\n",
    "        for n in range(len(neg)):\n",
    "            neg[n] = neg[n][1]\n",
    "        if env == 1:\n",
    "            Out = torch.cat((pOut, self.ecoForward(torch.tensor(neg)).reshape(-1, self.params['negNum_train'])), dim=1)\n",
    "            res = criterion(Out)[:, 0]\n",
    "            loss += torch.mean(res)\n",
    "        else:\n",
    "            Out = -torch.cat((pOut, self.ecoForward(torch.tensor(neg)).reshape(-1, self.params['negNum_train'])), dim=1)\n",
    "            res = criterion(Out)[:, 0]\n",
    "            loss += torch.mean(res)\n",
    "        return -loss\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.grads\n",
    "\n",
    "    def save_grad(self, name):\n",
    "        def hook(grad):\n",
    "            self.grads[name] = grad\n",
    "        return hook\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-chase",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch  1  training...\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                          | 0/740 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:   0%|                                                                            | 0/740 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:   1%|▊                                                                   | 9/740 [00:00<00:08, 88.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:   3%|█▊                                                                 | 20/740 [00:00<00:07, 99.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:   4%|██▋                                                                | 30/740 [00:00<00:07, 98.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:   6%|███▋                                                              | 41/740 [00:00<00:06, 101.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:   7%|████▋                                                             | 52/740 [00:00<00:06, 100.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:   9%|█████▋                                                             | 63/740 [00:00<00:06, 99.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  10%|██████▌                                                           | 74/740 [00:00<00:06, 100.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  11%|███████▌                                                          | 85/740 [00:00<00:06, 100.12it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  13%|████████▌                                                         | 96/740 [00:00<00:06, 101.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  14%|█████████▌                                                        | 107/740 [00:01<00:06, 97.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  16%|██████████▍                                                       | 117/740 [00:01<00:06, 90.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  17%|███████████▎                                                      | 127/740 [00:01<00:06, 88.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  19%|████████████▏                                                     | 137/740 [00:01<00:06, 90.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  20%|█████████████                                                     | 147/740 [00:01<00:06, 91.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  21%|██████████████                                                    | 158/740 [00:01<00:06, 94.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  23%|██████████████▉                                                   | 168/740 [00:01<00:06, 94.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  24%|███████████████▉                                                  | 179/740 [00:01<00:05, 97.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  26%|████████████████▉                                                 | 190/740 [00:01<00:05, 98.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  27%|█████████████████▊                                                | 200/740 [00:02<00:05, 97.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  28%|██████████████████▋                                               | 210/740 [00:02<00:05, 96.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  30%|███████████████████▌                                              | 220/740 [00:02<00:05, 94.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  31%|████████████████████▌                                             | 231/740 [00:02<00:05, 96.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  33%|█████████████████████▍                                            | 241/740 [00:02<00:05, 93.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  34%|██████████████████████▍                                           | 251/740 [00:02<00:05, 92.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  35%|███████████████████████▎                                          | 261/740 [00:02<00:05, 92.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  37%|████████████████████████▎                                         | 272/740 [00:02<00:04, 96.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  38%|█████████████████████████▏                                        | 282/740 [00:02<00:04, 94.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  39%|██████████████████████████                                        | 292/740 [00:03<00:04, 93.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  41%|██████████████████████████▉                                       | 302/740 [00:03<00:04, 92.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  42%|███████████████████████████▊                                      | 312/740 [00:03<00:04, 92.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  44%|████████████████████████████▊                                     | 323/740 [00:03<00:04, 95.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  45%|█████████████████████████████▋                                    | 333/740 [00:03<00:04, 94.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  46%|██████████████████████████████▌                                   | 343/740 [00:03<00:04, 92.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  48%|███████████████████████████████▍                                  | 353/740 [00:03<00:04, 93.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  49%|████████████████████████████████▍                                 | 363/740 [00:03<00:04, 91.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  50%|█████████████████████████████████▎                                | 373/740 [00:03<00:04, 91.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  52%|██████████████████████████████████▏                               | 383/740 [00:04<00:03, 89.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  53%|██████████████████████████████████▉                               | 392/740 [00:04<00:03, 88.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  54%|███████████████████████████████████▊                              | 402/740 [00:04<00:03, 89.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  56%|████████████████████████████████████▊                             | 413/740 [00:04<00:03, 92.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  57%|█████████████████████████████████████▋                            | 423/740 [00:04<00:03, 89.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  58%|██████████████████████████████████████▌                           | 432/740 [00:04<00:03, 86.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  60%|███████████████████████████████████████▎                          | 441/740 [00:04<00:03, 85.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  61%|████████████████████████████████████████▏                         | 450/740 [00:04<00:03, 86.36it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  62%|████████████████████████████████████████▉                         | 459/740 [00:04<00:03, 86.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  63%|█████████████████████████████████████████▋                        | 468/740 [00:05<00:03, 85.75it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  64%|██████████████████████████████████████████▌                       | 477/740 [00:05<00:03, 86.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  66%|███████████████████████████████████████████▎                      | 486/740 [00:05<00:02, 87.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  67%|████████████████████████████████████████████▏                     | 495/740 [00:05<00:02, 85.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  68%|████████████████████████████████████████████▉                     | 504/740 [00:05<00:02, 86.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  69%|█████████████████████████████████████████████▊                    | 513/740 [00:05<00:02, 86.51it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  71%|██████████████████████████████████████████████▌                   | 522/740 [00:05<00:02, 86.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  72%|███████████████████████████████████████████████▍                  | 532/740 [00:05<00:02, 90.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  73%|████████████████████████████████████████████████▎                 | 542/740 [00:05<00:02, 91.87it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  75%|█████████████████████████████████████████████████▏                | 552/740 [00:05<00:02, 91.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  76%|██████████████████████████████████████████████████                | 562/740 [00:06<00:01, 93.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  77%|███████████████████████████████████████████████████               | 572/740 [00:06<00:01, 92.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  79%|███████████████████████████████████████████████████▉              | 582/740 [00:06<00:01, 92.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  80%|████████████████████████████████████████████████████▊             | 592/740 [00:06<00:01, 90.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  81%|█████████████████████████████████████████████████████▋            | 602/740 [00:06<00:01, 88.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1:  83%|██████████████████████████████████████████████████████▍           | 611/740 [00:06<00:01, 87.07it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  84%|███████████████████████████████████████████████████████▎          | 620/740 [00:06<00:01, 85.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  85%|████████████████████████████████████████████████████████          | 629/740 [00:06<00:01, 86.79it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  86%|████████████████████████████████████████████████████████▉         | 638/740 [00:06<00:01, 84.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  87%|█████████████████████████████████████████████████████████▋        | 647/740 [00:07<00:01, 84.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  89%|██████████████████████████████████████████████████████████▌       | 657/740 [00:07<00:00, 88.22it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  90%|███████████████████████████████████████████████████████████▍      | 666/740 [00:07<00:00, 85.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  91%|████████████████████████████████████████████████████████████▏     | 675/740 [00:07<00:00, 84.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  92%|█████████████████████████████████████████████████████████████     | 684/740 [00:07<00:00, 84.82it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  94%|█████████████████████████████████████████████████████████████▊    | 693/740 [00:07<00:00, 84.20it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  95%|██████████████████████████████████████████████████████████████▌   | 702/740 [00:07<00:00, 84.72it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  96%|███████████████████████████████████████████████████████████████▌  | 712/740 [00:07<00:00, 86.91it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  97%|████████████████████████████████████████████████████████████████▎ | 721/740 [00:07<00:00, 86.88it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1:  99%|█████████████████████████████████████████████████████████████████ | 730/740 [00:08<00:00, 87.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 1: 100%|██████████████████████████████████████████████████████████████████| 740/740 [00:08<00:00, 90.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "epoch loss tensor(1584.2968, grad_fn=<AddBackward0>)\n",
      "Epoch  2  training...\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                          | 0/740 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:   0%|                                                                            | 0/740 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:   0%|▎                                                                   | 3/740 [00:00<00:25, 28.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:   2%|█                                                                  | 12/740 [00:00<00:12, 60.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:   3%|█▉                                                                 | 22/740 [00:00<00:09, 76.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:   4%|██▉                                                                | 32/740 [00:00<00:08, 82.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:   6%|███▊                                                               | 42/740 [00:00<00:07, 87.43it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:   7%|████▋                                                              | 52/740 [00:00<00:07, 89.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:   8%|█████▌                                                             | 62/740 [00:00<00:07, 90.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  10%|██████▌                                                            | 72/740 [00:00<00:07, 91.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  11%|███████▍                                                           | 82/740 [00:00<00:07, 91.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  12%|████████▎                                                          | 92/740 [00:01<00:06, 93.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  14%|█████████                                                         | 102/740 [00:01<00:06, 93.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  15%|█████████▉                                                        | 112/740 [00:01<00:07, 88.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  16%|██████████▊                                                       | 121/740 [00:01<00:06, 88.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  18%|███████████▌                                                      | 130/740 [00:01<00:07, 85.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  19%|████████████▍                                                     | 140/740 [00:01<00:06, 88.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  20%|█████████████▎                                                    | 149/740 [00:01<00:06, 85.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  21%|██████████████▏                                                   | 159/740 [00:01<00:06, 87.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  23%|██████████████▉                                                   | 168/740 [00:01<00:06, 86.92it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  24%|███████████████▉                                                  | 178/740 [00:02<00:06, 88.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  25%|████████████████▋                                                 | 187/740 [00:02<00:06, 86.80it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  26%|█████████████████▍                                                | 196/740 [00:02<00:06, 87.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  28%|██████████████████▎                                               | 206/740 [00:02<00:06, 88.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  29%|███████████████████▏                                              | 215/740 [00:02<00:06, 86.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  30%|████████████████████                                              | 225/740 [00:02<00:05, 88.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  32%|████████████████████▊                                             | 234/740 [00:02<00:05, 86.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  33%|█████████████████████▋                                            | 243/740 [00:02<00:05, 84.97it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  34%|██████████████████████▍                                           | 252/740 [00:02<00:05, 85.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  35%|███████████████████████▎                                          | 261/740 [00:03<00:05, 84.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  36%|████████████████████████                                          | 270/740 [00:03<00:05, 85.14it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "processed: 2:  38%|████████████████████████▉                                         | 279/740 [00:03<00:05, 84.42it/s]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "trainset = data_loader.TransactionData(train, userNum, itemNum, frequency)\n",
    "trainLoader = DataLoader(trainset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "testset = data_loader.UserTransactionData(test, userNum, itemNum, trainset.userHist)\n",
    "testset.set_negN(params['negNum_test'])\n",
    "testLoader = DataLoader(testset, batch_size=params['batch_size'], shuffle=False, num_workers=0)\n",
    "model = PT(userLen=userNum, itemLen=itemNum, distribution=frequency, params=params, item_price=itemprice)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['w_decay'])\n",
    "\n",
    "epoch = 0\n",
    "print('start training...')\n",
    "while epoch < params['epoch_limit']:\n",
    "    model.device = params['train_device']\n",
    "    model.to(model.device)\n",
    "\n",
    "    epoch += 1\n",
    "    print('Epoch ', str(epoch), ' training...')\n",
    "    L = len(trainLoader.dataset)\n",
    "    pbar = tqdm(total = L, file=sys.stdout)\n",
    "    pbar.set_description('processed: %d' % epoch)\n",
    "    for i, batchData in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        users = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        items = torch.LongTensor(batchData['item']).to(model.device)\n",
    "        negItems = torch.LongTensor(batchData['negItem']).reshape(-1).to(model.device)\n",
    "\n",
    "        batch_loss = model.loss(users, items, negItems)\n",
    "        batch_loss.backward()\n",
    "        grads = model.get_grads()\n",
    "        \n",
    "#         print('userBias_g:',grads['userBias_g'])\n",
    "#         print('itemBias_g:',grads['itemBias_g'])\n",
    "#         print('userEmbed_g:',grads['userEmbed_g'])\n",
    "#         print('itemEmbed_g:',grads['itemEmbed_g'])\n",
    "#         input()\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i == 0:\n",
    "            total_loss = batch_loss.clone()\n",
    "        else:\n",
    "            total_loss += batch_loss.clone()\n",
    "        pbar.update(users.shape[0])\n",
    "    pbar.close()\n",
    "    # torch.save(model, 'pt.pt')\n",
    "    print('epoch loss', total_loss)\n",
    "#     print(model.state_dict())\n",
    "\n",
    "    if epoch % params['test_per_train'] == 0:\n",
    "        print('starting val...')\n",
    "        model.device = params['test_device']\n",
    "        model.to(model.device)\n",
    "        L = len(testLoader.dataset)\n",
    "        pbar = tqdm(total=L, file=sys.stdout)\n",
    "        with torch.no_grad():\n",
    "            scoreDict = dict()\n",
    "            for i, batchData in enumerate(testLoader):\n",
    "#                 if np.random.random() < 0.98:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "#                 if i%50 != 0:\n",
    "#                     pbar.update(1)\n",
    "#                     continue\n",
    "                user = torch.LongTensor(batchData['user']).to(model.device)\n",
    "                posItems = torch.LongTensor(batchData['posItem']).to(model.device)\n",
    "                negItems = torch.LongTensor(batchData['negItem']).to(model.device)\n",
    "\n",
    "                items = torch.cat((posItems, negItems), 1).view(-1)\n",
    "                users = user.expand(items.shape[0])\n",
    "\n",
    "                score = model.forward(users, items)\n",
    "                scoreHeap = list()\n",
    "                for j in range(score.shape[0]):\n",
    "                    gt = False\n",
    "                    if j < posItems.shape[1]:\n",
    "                        gt = True\n",
    "\n",
    "                    heappush(scoreHeap, (1-score[j].cpu().numpy(), (0+items[j].cpu().numpy(), gt)))\n",
    "                scores = list()\n",
    "                candidate = len(scoreHeap)\n",
    "                for k in range(candidate):\n",
    "                    scores.append(heappop(scoreHeap))\n",
    "                pbar.update(1)\n",
    "                scoreDict[user[0]] = (scores, posItems.shape[1])\n",
    "        pbar.close()\n",
    "        testResult = evaluation.ranking_performance(scoreDict, 100)\n",
    "#         with open('./results/'+category+'/'+category+'_PT_valResult_'+str(epoch)+'.json', 'w') as outfile:\n",
    "#             json.dump(testResult, outfile)\n",
    "        \n",
    "print('starting test...')\n",
    "model.device = params['test_device']\n",
    "model.to(model.device)\n",
    "L = len(testLoader.dataset)\n",
    "pbar = tqdm(total=L, file=sys.stdout)\n",
    "alpha = []\n",
    "beta = []\n",
    "lamda = []\n",
    "gamma = []\n",
    "delta = []\n",
    "ref = []\n",
    "with torch.no_grad():\n",
    "    scoreDict = dict()\n",
    "    ecoDict = dict()\n",
    "    for i, batchData in enumerate(testLoader):\n",
    "        user = torch.LongTensor(batchData['user']).to(model.device)\n",
    "        posItems = torch.LongTensor(batchData['posItem']).to(model.device)\n",
    "        negItems = torch.LongTensor(batchData['negItem']).to(model.device)\n",
    "\n",
    "        items = torch.cat((posItems, negItems), 1).view(-1)\n",
    "        users = user.expand(items.shape[0])\n",
    "        \n",
    "        #[a,b,l,g,d,r] = model.get_paras(users,items)\n",
    "    \n",
    "        \n",
    "        #alpha.append(a.cpu().numpy())\n",
    "        #beta.append(b.cpu().numpy())\n",
    "        #lamda.append(l.cpu().numpy())\n",
    "        #gamma.append(g.cpu().numpy())\n",
    "        #delta.append(d.cpu().numpy())\n",
    "        #ref.append(r.cpu().numpy())\n",
    "        \n",
    "        score = model.forward(users, items)\n",
    "        scoreHeap = list()\n",
    "        ecoHeap = list()\n",
    "        for j in range(score.shape[0]):\n",
    "            gt = False\n",
    "            et = False\n",
    "            if j < posItems.shape[1]:\n",
    "                gt = True\n",
    "            for k in data_loader.items:\n",
    "                if (data_loader.items[k][1] == items[j]):\n",
    "                    if data_loader.items[k][0] == 1:\n",
    "                        et = True\n",
    "            heappush(scoreHeap, (1 - score[j].cpu().numpy(), (0 + items[j].cpu().numpy(), gt)))\n",
    "            heappush(ecoHeap, (1 - score[j].cpu().numpy(), (0 + items[j].cpu().numpy(), et)))\n",
    "        scores = list()\n",
    "        ecoScores = list()\n",
    "        candidate = len(scoreHeap)\n",
    "        for k in range(candidate):\n",
    "            scores.append(heappop(scoreHeap))\n",
    "            ecoScores.append(heappop(ecoHeap))\n",
    "        pbar.update(1)\n",
    "        scoreDict[user[0]] = (scores, posItems.shape[1])\n",
    "        ecoDict[user[0]] = (ecoScores, posItems.shape[1])\n",
    "pbar.close()\n",
    "testResult = evaluation.ranking_performance(scoreDict, params['negNum_test']+1)\n",
    "print(\"ECO-RESULTS\")\n",
    "testResult = evaluation.ranking_performance(ecoDict, params['negNum_test']+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-occupation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
